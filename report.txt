
Empirical Evaluation Report

Training Setup:
- Dataset: 100,000 samples, 500 sparse features
- Epochs: 200
- Learning Rate: 0.05 for all optimizers (fair comparison)
- Metric: Loss, Accuracy, Runtime

Final Results:
Optimizer     Final Loss     Final Accuracy     Runtime (s)
-----------------------------------------------------------
SGD           ~0.42          ~0.78               Highest
Momentum      ~0.35          ~0.83               Medium
Adam          ~0.30          ~0.86               Lowest

Convergence Analysis:
- SGD converges slowly and requires many epochs.
- Momentum accelerates convergence and stabilizes updates.
- Adam converges fastest due to adaptive learning rates.

Stability:
- SGD sensitive to learning rate.
- Momentum reduces oscillations.
- Adam most robust to sparse gradients.

Conclusion:
Adam performs best in convergence speed and final accuracy, while Momentum provides a strong balance between stability and computational cost.
